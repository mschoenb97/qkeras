# -*- coding: utf-8 -*-
"""Flax Dying Relu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dauAr6TQsyfY7zL0HAUpF6t8y2TCaLlw
"""

import jax
import jax.numpy as jnp
from flax import linen as nn
from flax.training import train_state
from flax.training.common_utils import get_metrics
from jax import random
from matplotlib import pyplot as plt
import numpy as np
import optax
from itertools import product
from functools import lru_cache

import pandas as pd


from tqdm import tqdm
from jax import custom_jvp, jvp, vjp, grad, custom_vjp

"""# Models"""

# Define the model
class MLP(nn.Module):

  activation: callable
  rank: int

  def setup(self):
    self.layers = [nn.Dense(2 * self.rank, kernel_init=nn.initializers.he_normal()) for _ in range(10 * self.rank)]
    self.final = nn.Dense(self.rank, kernel_init=nn.initializers.he_normal())

  def __call__(self, x):
    for i, layer in enumerate(self.layers):
      x = layer(x)
      if i != len(self.layers) - 1: # no activation after the last layer
        x = self.activation(x)
    return self.final(x)

class MLPSparsity(nn.Module):

  activation: callable
  rank: int

  def setup(self):
    self.layers = [nn.Dense(2 * self.rank, kernel_init=nn.initializers.he_normal()) for _ in range(10 * self.rank)]
    self.final = nn.Dense(self.rank, kernel_init=nn.initializers.he_normal())

  def __call__(self, x):
    sparse_elts = 0
    total_elts = 0
    for i, layer in enumerate(self.layers):
      x = layer(x)
      if i != len(self.layers) - 1: # no activation after the last layer
        x = self.activation(x)
        total_elts += x.size
        sparse_elts += jnp.sum(x == 0)
    return sparse_elts / total_elts

"""# Activations"""

class ZombieReLU:

  def __init__(self, right_slope):

    self.right_slope = right_slope

    @custom_vjp
    def sided_relu(x):
      return nn.relu(x)

    def sided_relu_fwd(x):
      return sided_relu(x), (x,)

    def sided_relu_bwd(res, g):
      x, = res # Gets residuals computed in f_fwd

      ones = jnp.ones_like(x)
      left_gradient = jnp.where(x > 0, ones, jnp.zeros_like(x))
      right_gradient = jnp.where(x > 0, ones, right_slope * ones)
      tangent_out = jnp.where(g < 0, right_gradient, left_gradient) * g
      # tangent_out = 0.0 * g
      return (tangent_out, )

    sided_relu.defvjp(sided_relu_fwd, sided_relu_bwd)

    self.sided_relu = sided_relu

  def __call__(self, x):
    return self.sided_relu(x)


class FakeZombieReLU:

  def __init__(self, negative_slope=0.01):

    self.negative_slope = negative_slope

    @custom_vjp
    def relu(x):
      return nn.relu(x)

    def relu_fwd(x):
      return relu(x), (x,)

    def relu_bwd(res, g):
      x, = res # Gets residuals computed in f_fwd

      ones = jnp.ones_like(x)
      tangent_out = jnp.where(x > 0, ones, negative_slope * ones) * g
      return (tangent_out, )

    relu.defvjp(relu_fwd, relu_bwd)

    self.relu = relu

  def __call__(self, x):
    return self.relu(x)

class LeakyReLU:

  def __init__(self, negative_slope=0.01):
    self.negative_slope = negative_slope

  def __call__(self, x):

    return jnp.where(x >= 0, x, self.negative_slope * x)

"""# Helper Functions"""

def get_data(n_samples, key, target_func, rank):
  rng = jax.random.PRNGKey(key)
  # Create inputs uniform between -sqrt(3) and sqrt(3)
  x = jax.random.uniform(rng, shape=(n_samples, rank), minval=-jnp.sqrt(3), maxval=jnp.sqrt(3))
  # Create dummy targets with the same shape as inputs
  y = target_func(x)
  return x, y

def get_output_counts(x, y, target_func, rank):

  correct_count = (np.abs(target_func(x) - y) < .01).sum()

  y_min, y_max = y.min(), y.max()

  if rank == 1:
    x_diff = x[1:] - x[:-1]
    y_diff = y[1:] - y[:-1]

    no_slope_count = (np.abs(y_diff / x_diff) < .01).sum()
  else: # mutli-dimensional output
    if abs(y_min - y_max) < .01:
      no_slope_count = x.size
    else:
      no_slope_count = 0

  return correct_count, no_slope_count, y_max - y_min, x.size


def classify_output(loss, correct_count, no_slope_count, range, n):

  # return correct_count, no_slope_count

  if loss < .001:
    return 'success'

  if no_slope_count > .99 * n:
    return 'collapsed'

  if correct_count + no_slope_count > .9 * n:
    return 'half_trained'

  if correct_count < 100:
    return 'untrained'

  if range > .01:
    return 'not_collapsed'

  return 'unclassified'

"""# Training"""

def get_model_params(model, activation, key, target_func, rank, track=False, num_epochs=20,
                     patience=5, min_delta=1e-5, debug=False):

  # Squared error loss function
  @jax.jit
  def compute_loss(params, batch):
    inputs, targets = batch
    predictions = model(activation, rank).apply(params, inputs)
    return jnp.mean((predictions - targets)**2)

  # Define training step
  @jax.jit
  def train_step(state, batch):
    def loss_fn(params):
      return compute_loss(params, batch)

    grad = jax.grad(loss_fn)(state.params)
    return state.apply_gradients(grads=grad)

  # Initialize model and optimizer
  rng = random.PRNGKey(key)
  x = jnp.ones((64, rank)) # initial input batch
  params = model(activation, rank).init(rng, x)
  optimizer = optax.adam(learning_rate=0.001)
  state = train_state.TrainState.create(apply_fn=model(activation, rank).apply, params=params, tx=optimizer)

  # Specify parameters
  num_data_points = 3000
  batch_size = 64

  # Get the data
  x_data, y_data = get_data(num_data_points, key, target_func, rank)

  # Prepare for training
  num_batches = num_data_points // batch_size

  if track:
    epochs_loop = tqdm(range(1, num_epochs + 1))
  else:
    epochs_loop = range(1, num_epochs + 1)

  losses = []
  epochs_no_improve = 0

  # Training loop
  for epoch in epochs_loop:

    epoch_loss = 0

    for batch_idx in range(num_batches):
      start = batch_idx * batch_size
      end = start + batch_size
      x_batch = x_data[start:end]
      y_batch = y_data[start:end]
      state = train_step(state, (x_batch, y_batch))

      batch_loss = compute_loss(state.params, (x_batch, y_batch))
      epoch_loss += batch_loss / num_batches


    if len(losses) > 0 and abs(epoch_loss - losses[-1]) > min_delta:
      epochs_no_improve = 0
    else:
      epochs_no_improve += 1

    # Save loss for this epoch
    losses.append(epoch_loss)
    if debug:
      print(f'epoch = {epoch}, epoch_loss = {epoch_loss}')

    # Check for early stopping condition
    if epochs_no_improve >= patience:
      if debug:
        print("Early stopping after {} epochs".format(epoch))
      break

  return state.params, epoch

"""# Data Collection"""

def get_collapsing_data(model, activation, key, target_func, rank, display=False,
                        track=True, num_epochs=20, debug=False, min_delta=1e-5):

  params, stopping_epoch = get_model_params(model, activation, key, target_func, rank,
                                            track=track, num_epochs=num_epochs, debug=debug, min_delta=min_delta)

  def predict(params, inputs):
    return model(activation, rank).apply(params, inputs)

  # Generate some new data
  n_samples = 1000
  if display and rank == 2:
    n_samples = 30
  if rank == 1:
    x_new = jnp.linspace(-jnp.sqrt(3), jnp.sqrt(3), num=n_samples).reshape((-1, rank))
  else:
    assert rank == 2
    x = jnp.linspace(-jnp.sqrt(3), jnp.sqrt(3), num=n_samples)
    y = jnp.linspace(-jnp.sqrt(3), jnp.sqrt(3), num=n_samples)
    X, Y = np.meshgrid(x, y)
    x_new = np.stack([X.reshape(-1), Y.reshape(-1)], axis=-1)


  # Predict the output for the new data
  y_pred = predict(params, x_new)

  y_min = y_pred.min()
  y_max = y_pred.max()
  unique_count = len(np.unique(np.array(y_pred)))

  predictions = model(activation, rank).apply(params, x_new)
  loss = jnp.mean((predictions - target_func(x_new))**2)

  sparsity = MLPSparsity(activation, rank).apply(params, x_new)

  correct_count, no_slope_count, range, n = get_output_counts(x_new, y_pred, target_func, rank)
  label = classify_output(loss, correct_count, no_slope_count, range, n)

  if display:
    if rank == 1:
      plt.plot(x_new, y_pred, label='Predicted')
      plt.plot(x_new, target_func(x_new), label='Target')
      print('Min: ', y_min)
      print('Max: ', y_max)
      print('loss: ', loss)
      print('Unique output values: ', unique_count)
    else:
      fig = plt.figure()
      ax = fig.add_subplot(111, projection='3d')

      ax.scatter(x_new[:,0], x_new[:,1], y_pred[:,0])
      ax.scatter(x_new[:,0], x_new[:,1], target_func(x_new)[:,0])

  res = {
    'range': y_max - y_min,
    'unique_count': unique_count,
    'loss': loss,
    'stopping_epoch': stopping_epoch,
    'sparsity': sparsity,
    'correct_count': correct_count,
    'no_slope_count': no_slope_count,
    'label': label,
    'target_func': target_func.__name__,
  }

  if debug:
    return res, x_new, y_pred
  else:
    return res

"""# Target functions"""

def f1(x):

  return jnp.abs(x)

def f2(x):

  return x * jnp.sin(5 * x)

def f3(x):

  return (jnp.sign(x) + 1.0) / 2.0 + 0.2 * jnp.sin(5 * x)

def f4(x):

  return np.vstack([np.abs(x[:,0] + x[:,1]), np.abs(x[:,0] - x[:,1])]).T


@lru_cache()
def get_all_collapsing_data(models, activations, min_key, max_key, target_funcs, num_epochs=20):

  res = []

  loop_items = [
      models,
      activations,
      target_funcs,
      range(min_key, max_key),
  ]

  loop = list(product(*loop_items))

  for model, activation, target_func, key in tqdm(loop):
    elt = {
      'model': model,
      'activation': activation,
      'target_func': target_func.__name__,
      'key': key,
    }
    rank = 2 if target_func.__name__ == 'f4' else 1
    model = eval(model)
    activation = eval(activation)
    data = get_collapsing_data(model, activation, key, target_func, rank, track=False, num_epochs=num_epochs)
    elt.update(data)
    res.append(dict(elt))

  return pd.DataFrame(res)


if __name__ == '__main__':
  models = ('MLP',)
  activations = (
      'nn.relu',
      'ZombieReLU(right_slope=0.01)',
      'LeakyReLU(negative_slope=0.01)',
  )
  target_funcs = (
      f1,
      f2,
      f3,
      f4,
  )


  min_key = 0
  max_key = 1000
  num_epochs = 250

  res = get_all_collapsing_data(models, activations, min_key, max_key, target_funcs, num_epochs=num_epochs)

  res.to_csv('dying_relu_data.csv')
